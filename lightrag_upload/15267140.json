{
  "date_produced": "20180228",
  "publication_number": "US20180075347A1-20180315",
  "main_ipcr_label": "G06N308",
  "decision": "PENDING",
  "application_number": "15267140",
  "inventor_list": [
    {
      "inventor_name_last": "Alistarh",
      "inventor_name_first": "Dan",
      "inventor_city": "Geneva",
      "inventor_state": "",
      "inventor_country": "CH"
    },
    {
      "inventor_name_last": "Li",
      "inventor_name_first": "Jerry Zheng",
      "inventor_city": "Issaquah",
      "inventor_state": "WA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Tomioka",
      "inventor_name_first": "Ryota",
      "inventor_city": "Cambridge",
      "inventor_state": "",
      "inventor_country": "GB"
    },
    {
      "inventor_name_last": "Vojnovic",
      "inventor_name_first": "Milan",
      "inventor_city": "Cambridge",
      "inventor_state": "",
      "inventor_country": "GB"
    }
  ],
  "abstract": "A computation node of a neural network training system is described. The node has a memory storing a plurality of gradients of a loss function of the neural network and an encoder. The encoder encodes the plurality of gradients by setting individual ones of the gradients either to zero or to a quantization level according to a probability related to at least the magnitude of the individual gradient. The node has a processor which sends the encoded plurality of gradients to one or more other computation nodes of the neural network training system over a communications network.",
  "filing_date": "20160915",
  "patent_number": "None",
  "summary": "<SOH> SUMMARY <EOH>The following presents a simplified summary of the disclosure in order to provide a basic understanding to the reader. This summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter. Its sole purpose is to present a selection of concepts disclosed herein in a simplified form as a prelude to the more detailed description that is presented later. A computation node of a neural network training system is described. The node has a memory storing a plurality of gradients of a loss function of the neural network and an encoder. The encoder encodes the plurality of gradients by setting individual ones of the gradients either to zero or to a quantization level according to a probability related to at least the magnitude of the individual gradient. The node has a processor which sends the encoded plurality of gradients to one or more other computation nodes of the neural network training system over a communications network. Many of the attendant features will be more readily appreciated as the same becomes better understood by reference to the following detailed description considered in connection with the accompanying drawings.",
  "date_published": "20180315",
  "title": "EFFICIENT TRAINING OF NEURAL NETWORKS",
  "ipcr_labels": [
    "G06N308"
  ],
  "_processing_info": {
    "original_size": 64972,
    "optimized_size": 2817,
    "reduction_percent": 95.66
  }
}