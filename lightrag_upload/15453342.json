{
  "patent_number": "None",
  "application_number": "15453342",
  "date_published": "20180503",
  "date_produced": "20180418",
  "filing_date": "20170308",
  "main_ipcr_label": "G06N504",
  "abstract": "Improvements in speed and reductions in computational resource expenditure are realized in the improved tuning of hyperparameters for machine learning processes. To ensure that the values selected for hyperparameters are tuned appropriately, but quickly, several rounds of optimization are performed, each with as many or more iterations of cross-validation than prior rounds; cutting short the analysis unpromising results to devote more time and resources in analyzing promising value sets. The results are used to build suggested sets of hyperparameter values for that round, which are also cross-validated and enable the tuning process to incorporate previous operations to improve its value sets. The most promising sets of hyperparameter values from each round are selected as the basis set for the next round until a final set of values for the hyperparameters is developed.",
  "publication_number": "US20180121814A1-20180503",
  "summary": "<SOH> SUMMARY <EOH>This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description section. This summary is not intended to identify all key or essential features of the claimed subject matter, nor is it intended as an aid in determining the scope of the claimed subject matter. Systems and methods are provided herein to enable improvements in speed and reductions in the computing resources expended when tuning hyperparameters. As described herein, knowledge from previous evaluations of candidate value sets for the hyperparameters are incorporated into the analysis and growth of the candidate value sets while directing the expenditure of computing resources to those candidate value sets that are most promising. Candidate value sets for the hyperparameters are selected and compared against a training model over several rounds of analysis. During each round, the selected value sets are evaluated against a training model and are used to develop suggested value sets in conjunction with a running knowledge of the analyzed value sets, which are also evaluated against the training model and used to update the running knowledge. At the end of each round, the most accurate value sets (whether initially selected or suggested) are selected as the value sets to be analyzed in the next round. Each successive round analyses fewer candidate value sets, but provides a greater analysis into those candidate value sets selected by further iterating their models against the training data. In this way, the analysis of unpromising candidate value sets for the hyperparameters is cut short, and the running knowledge is incorporated sooner into the tuning, thus improving the speed and efficiency of the machines used to tune hyperparameters. Examples are implemented as a computer process, a computing system, or as an article of manufacture such as a device, computer program product, or computer readable medium. Ac...",
  "ipcr_labels": [
    "G06N504",
    "G06N9900"
  ],
  "inventor_list": [
    {
      "inventor_name_last": "Yu",
      "inventor_name_first": "Dong",
      "inventor_city": "Bothell",
      "inventor_state": "WA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Jin",
      "inventor_name_first": "Chi",
      "inventor_city": "Foster City",
      "inventor_state": "NJ",
      "inventor_country": "US"
    }
  ],
  "title": "HYPERPARAMETER TUNING",
  "decision": "PENDING",
  "_processing_info": {
    "original_size": 60843,
    "optimized_size": 3538,
    "reduction_percent": 94.19
  }
}