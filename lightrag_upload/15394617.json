{
  "date_produced": "20170719",
  "publication_number": "US20170220925A1-20170803",
  "main_ipcr_label": "G06N304",
  "decision": "PENDING",
  "application_number": "15394617",
  "inventor_list": [
    {
      "inventor_name_last": "Alsharif",
      "inventor_name_first": "Ouais",
      "inventor_city": "Mountain View",
      "inventor_state": "CA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Prabhavalkar",
      "inventor_name_first": "Rohit Prakash",
      "inventor_city": "Santa Clara",
      "inventor_state": "CA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "McGraw",
      "inventor_name_first": "Ian C.",
      "inventor_city": "Menlo Park",
      "inventor_state": "CA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Bruguier",
      "inventor_name_first": "Antoine Jean",
      "inventor_city": "Milpitas",
      "inventor_state": "CA",
      "inventor_country": "US"
    }
  ],
  "abstract": "Methods, systems, and apparatus, including computer programs encoded on computer storage media, for implementing a compressed recurrent neural network (RNN). One of the systems includes a compressed RNN, the compressed RNN comprising a plurality of recurrent layers, wherein each of the recurrent layers has a respective recurrent weight matrix and a respective inter-layer weight matrix, and wherein at least one of recurrent layers is compressed such that a respective recurrent weight matrix of the compressed layer is defined by a first compressed weight matrix and a projection matrix and a respective inter-layer weight matrix of the compressed layer is defined by a second compressed weight matrix and the projection matrix.",
  "filing_date": "20161229",
  "patent_number": "None",
  "summary": "<SOH> SUMMARY <EOH>This specification describes technologies that relate to recurrent neural network architectures. In general, a recurrent neural network includes at least one recurrent neural network layer that is compressed. In particular, the recurrent weight matrix and the inter-layer weight matrix for the compressed recurrent layer are jointly compressed using a shared projection matrix. For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions. For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions. The subject matter described in this specification can be implemented in particular embodiments so as to realize one or more of the following advantages. By compressing the weight matrices of one or more of the recurrent layers in a recurrent neural network, the recurrent neural network is configured to be able to process data more efficiently and use less data storage. In particular, a recurrent neural network having one or compressed recurrent layers can be effectively trained to achieve performance that is comparable to full size, e.g., uncompressed, recurrent neural networks, while using less data storage and being able to process inputs faster by virtue of the compressed weight matrices of the compressed recurrent layers having fewer parameters than the weight matrices of the corresponding layers in the uncompressed recurrent neural network. In fact, because the compressed recurrent neural network has a smaller computational footprint, the compressed network may be able to be effectively implemented to process inputs in real-time on a mobile device havi...",
  "date_published": "20170803",
  "title": "COMPRESSED RECURRENT NEURAL NETWORK MODELS",
  "ipcr_labels": [
    "G06N304",
    "G06N308"
  ],
  "_processing_info": {
    "original_size": 47567,
    "optimized_size": 3742,
    "reduction_percent": 92.13
  }
}