{
  "date_produced": "20180117",
  "publication_number": "US20180032871A1-20180201",
  "main_ipcr_label": "G06N308",
  "decision": "PENDING",
  "application_number": "15222997",
  "inventor_list": [
    {
      "inventor_name_last": "Holt",
      "inventor_name_first": "Jason E.",
      "inventor_city": "Mountain View",
      "inventor_state": "CA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Herreshoff",
      "inventor_name_first": "Marcello",
      "inventor_city": "Palo Alto",
      "inventor_state": "CA",
      "inventor_country": "US"
    }
  ],
  "abstract": "The present disclosure provides systems and methods that enable training of an encoder model based on a decoder model that performs an inverse transformation relative to the encoder model. In one example, an encoder model can receive a first set of inputs and output a first set of outputs. The encoder model can be a neural network. The decoder model can receive the first set of outputs and output a second set of outputs. A loss function can describe a difference between the first set of inputs and the second set of outputs. According to an aspect of the present disclosure, the loss function can be sequentially backpropagated through the decoder model without modifying the decoder model and then through the encoder model while modifying the encoder model, thereby training the encoder model. Thus, an encoder model can be trained to have enforced consistency relative to the inverse decoder model.",
  "filing_date": "20160729",
  "patent_number": "None",
  "summary": "<SOH> SUMMARY <EOH>Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or can be learned from the description, or can be learned through practice of the embodiments. One example aspect of the present disclosure is directed to a computer-implemented method to perform machine learning. The method includes obtaining, by one or more computing devices, data descriptive of an encoder model that is configured to receive a first set of inputs and, in response to receipt of the first set of inputs, output a first set of outputs. The method includes obtaining, by the one or more computing devices, data descriptive of a decoder model that is configured to receive the first set of outputs and, in response to receipt of the first set of outputs, output a second set of outputs. The method includes determining, by the one or more computing devices, a loss function that describes a difference between the first set of inputs and the second set of outputs. The method includes backpropagating, by the one or more computing devices, the loss function through the decoder model without modifying the decoder model. The method includes, after backpropagating, by the one or more computing devices, the loss function through the decoder model, continuing to backpropagate, by the one or more computing devices, the loss function through the encoder model to train the encoder model. Continuing to backpropagate, by the one or more computing devices, the loss function through the encoder model to train the encoder model includes adjusting, by the one or more computing devices, at least one weight included in the encoder model. Another example aspect of the present disclosure is directed to a computing system to perform machine learning. The computing system includes at least one processor and at least one tangible, non-transitory computer-readable medium that stores instructions that, when executed by the at least one processor, ca...",
  "date_published": "20180201",
  "title": "Systems and Methods to Perform Machine Learning with Feedback Consistency",
  "ipcr_labels": [
    "G06N308"
  ],
  "_processing_info": {
    "original_size": 88791,
    "optimized_size": 3625,
    "reduction_percent": 95.92
  }
}