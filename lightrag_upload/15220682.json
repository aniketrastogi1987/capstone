{
  "date_produced": "20170316",
  "publication_number": "US20170091668A1-20170330",
  "main_ipcr_label": "G06N9900",
  "decision": "PENDING",
  "application_number": "15220682",
  "inventor_list": [
    {
      "inventor_name_last": "Kadav",
      "inventor_name_first": "Asim",
      "inventor_city": "Franklin Park",
      "inventor_state": "NJ",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Kruus",
      "inventor_name_first": "Erik",
      "inventor_city": "Hillsborough",
      "inventor_state": "NJ",
      "inventor_country": "US"
    }
  ],
  "abstract": "A machine learning method includes connecting machines in a data-center using a network aware model consistency for stochastic applications; ensuring a communication graph of all machines in the data-center is connected; propagating all updates uniformly across the cluster without update; and preferring connections to a machine with first network throughput over machines with second network throughput smaller than the first network throughput.",
  "filing_date": "20160727",
  "patent_number": "None",
  "summary": "<SOH> SUMMARY <EOH>In one aspect, a machine learning method includes connecting machines in a data-center using a network aware model consistency for stochastic applications; ensuring a communication graph of all machines in the data-center is connected; propagating all updates uniformly across the cluster without update; and first preferring connections to a machine with higher network throughput over machines with second, lower network throughput than the first network throughput. Advantages of the preferred embodiments may include one or more of the following. The system takes into consideration network latency or speeds between the attached machines. The system works with a partial version of all-reduce based primitives, where machines may communicate with fewer machines to limit communication costs while considering network throughput. We call this as sparse-reduce primitive, where machines may perform reduce with fewer machines while ensuring that the network graph of machines remains connected. Balancing CPU and network provides an efficient system that trains machine-learning models quickly and with low running costs. Ensuring all replicas converge at the same time, improves model accuracy. More accurate models, with faster training times ensures that all NEC businesses and applications such as job recommendations, internet helpdesks, etc. provide more accurate results.",
  "date_published": "20170330",
  "title": "SYSTEM AND METHOD FOR NETWORK BANDWIDTH AWARE DISTRIBUTED LEARNING",
  "ipcr_labels": [
    "G06N9900",
    "H04L2908"
  ],
  "_processing_info": {
    "original_size": 35090,
    "optimized_size": 2561,
    "reduction_percent": 92.7
  }
}