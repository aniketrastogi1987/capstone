{
  "patent_number": "None",
  "application_number": "15789614",
  "date_published": "20180503",
  "date_produced": "20180418",
  "filing_date": "20171020",
  "main_ipcr_label": "G06N300",
  "abstract": "A context-aware attention-based neural network is provided for answering an input question given a set of purportedly supporting statements for the input question. The neural network includes a processing element. The processing element is configured to calculate a question representation for the input question, based on word annotations and word-level attentions calculated for the input question. The processing element is further configured to calculate a sentence representation for each of the purportedly supporting statements, based on word annotations and word-level attentions calculated for each of the purportedly supporting statements. The processing element is also configured to calculate a context representation for the set of purportedly supporting statements with respect to the sentence representation for each of the purportedly supporting statements. The processing element is additionally configured to generate an answer to the input question based on the question representa...",
  "publication_number": "US20180121785A1-20180503",
  "summary": "<SOH> SUMMARY <EOH>According to an aspect of the present invention, a context-aware attention-based neural network is provided for answering an input question given a set of purportedly supporting statements for the input question. The neural network includes a processing element. The processing element is configured to calculate a question representation for the input question, based on word annotations and word-level attentions calculated for the input question. The processing element is further configured to calculate a sentence representation for each of the purportedly supporting statements, based on word annotations and word-level attentions calculated for each of the purportedly supporting statements. The processing element is also configured to calculate a context representation for the set of purportedly supporting statements with respect to the sentence representation for each of the purportedly supporting statements. The processing element is additionally configured to generate an answer to the input question based on the question representation and the context representation. According to another aspect of the present invention, a computer program product is provided for implementing a context-aware attention-based neural network for answering an input question given a set of purportedly supporting statements for the input question. The computer program product includes a non-transitory computer readable storage medium having program instructions embodied therewith. The program instructions are executable by a computer to cause the computer to perform a method. The method includes calculating, by a processing element of the computer, a question representation for the input question, based on word annotations and word-level attentions calculated for the input question. The method further includes calculating, by the processing element, a sentence representation for each of the purportedly supporting statements, based on word annotations and word-level att...",
  "ipcr_labels": [
    "G06N300",
    "G06N304"
  ],
  "inventor_list": [
    {
      "inventor_name_last": "Min",
      "inventor_name_first": "Renqiang",
      "inventor_city": "Princeton",
      "inventor_state": "NJ",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Kadav",
      "inventor_name_first": "Asim",
      "inventor_city": "Jersey City",
      "inventor_state": "NJ",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Li",
      "inventor_name_first": "Huayu",
      "inventor_city": "Charlotte",
      "inventor_state": "NC",
      "inventor_country": "US"
    }
  ],
  "title": "CONTEXT-AWARE ATTENTION-BASED NEURAL NETWORK FOR INTERACTIVE QUESTION ANSWERING",
  "decision": "PENDING",
  "_processing_info": {
    "original_size": 53019,
    "optimized_size": 3869,
    "reduction_percent": 92.7
  }
}