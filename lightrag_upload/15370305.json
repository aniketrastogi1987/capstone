{
  "date_produced": "20180516",
  "publication_number": "US20180150743A1-20180531",
  "main_ipcr_label": "G06N308",
  "decision": "PENDING",
  "application_number": "15370305",
  "inventor_list": [
    {
      "inventor_name_last": "Ma",
      "inventor_name_first": "Fenglong",
      "inventor_city": "Amherst",
      "inventor_state": "NY",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Chitta",
      "inventor_name_first": "Radha",
      "inventor_city": "Webster",
      "inventor_state": "NY",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Zhou",
      "inventor_name_first": "Jing",
      "inventor_city": "Pittsford",
      "inventor_state": "NY",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Ramesh",
      "inventor_name_first": "Palghat S.",
      "inventor_city": "Pittsford",
      "inventor_state": "NY",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Sun",
      "inventor_name_first": "Tong",
      "inventor_city": "Penfield",
      "inventor_state": "NY",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Kataria",
      "inventor_name_first": "Saurabh Singh",
      "inventor_city": "Sunnyvale",
      "inventor_state": "CA",
      "inventor_country": "US"
    }
  ],
  "abstract": "A long-term memory network method and system for text comprehension. A recurrent neural network can be provided, which includes an external memory module and a long-short term memory unit, wherein said recurrent neural network encodes raw text information into vector representations, forms memories, finds relevant sentences to answer questions, and generates multi-word answers to said questions utilizing the long short term memory unit.",
  "filing_date": "20161206",
  "patent_number": "None",
  "summary": "<SOH> BRIEF SUMMARY <EOH>The following summary is provided to facilitate an understanding of some of the innovative features unique to the disclosed embodiments and is not intended to be a full description. A full appreciation of the various aspects of the embodiments disclosed herein can be gained by taking the entire specification, claims, drawings, and abstract as a whole. It is, therefore, one aspect of the disclosed embodiments to provide for a long-term memory network that improves question-answering technology. It is another aspect of the disclosed embodiments to provide for a long-term memory network architecture that combines two neural network architectures, an end-to-end memory network, and a long short-term memory network to generate multiple word answers to textual questions. It is yet another aspect of the disclosed embodiments to provide for a long-term memory network that can be easily trained end-to-end with minimal data and supervision. It is still another aspect of the disclosed embodiments to provide for a long-term memory network that can be utilized to extract knowledge from any generic set of articles/publications, given questions pertaining to such articles. It is another aspect of the disclosed embodiments to provide for a neural network that can learn word embeddings from ontologies and other database. The aforementioned aspects and other objectives and advantages can now be achieved as described herein. A long-term memory network (LTMN) method and system for text comprehension is disclosed, which includes a recurrent neural network comprising a LSTM (long-short term memory) unit. The recurrent neural network can be configured to encode raw text information into vector representations, form memories, find relevant sentences to answer questions, and generate multi-word answers to the questions utilizing the long-short term memory unit(s). The LTMN can incorporate both an external memory module and the LSTM units/modules to comprehend the inp...",
  "date_published": "20180531",
  "title": "LONG-TERM MEMORY NETWORKS FOR KNOWLEDGE EXTRACTION FROM TEXT AND PUBLICATIONS",
  "ipcr_labels": [
    "G06N308",
    "G06N304"
  ],
  "_processing_info": {
    "original_size": 72569,
    "optimized_size": 3747,
    "reduction_percent": 94.84
  }
}