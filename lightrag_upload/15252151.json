{
  "date_produced": "20170726",
  "publication_number": "US20170228646A1-20170810",
  "main_ipcr_label": "G06N308",
  "decision": "PENDING",
  "application_number": "15252151",
  "inventor_list": [
    {
      "inventor_name_last": "O'CONNOR",
      "inventor_name_first": "Peter",
      "inventor_city": "Amsterdam",
      "inventor_state": "",
      "inventor_country": "NL"
    },
    {
      "inventor_name_last": "WELLING",
      "inventor_name_first": "Max",
      "inventor_city": "Amsterdam",
      "inventor_state": "",
      "inventor_country": "NL"
    }
  ],
  "abstract": "A method of training a neural network with back propagation includes generating error events representing a gradient of a cost function for the neural network. The error events may be generated based on a forward pass through the neural network resulting from input events, weights of the neural network and events from a target signal. The method further includes updating the weights of the neural network based on the error events.",
  "filing_date": "20160830",
  "patent_number": "None",
  "summary": "<SOH> SUMMARY <EOH>In an aspect of the present disclosure, a method of training a neural network with back propagation is presented. The method includes generating error events representing a gradient of a cost function for the neural network. The error events are generated based on a forward pass through the neural network resulting from input events, weights of the neural network and events from a target signal. The method also includes updating the weights of the neural network based on the error events. In another aspect of the present disclosure, an apparatus for training a neural network with back propagation is presented. The apparatus includes a memory and at least one processor coupled to the memory. The one or more processors are configured to generate error events representing a gradient of a cost function for the neural network. The error events are generated based on a forward pass through the neural network resulting from input events, weights of the neural network and events from a target signal. The processor(s) is(are) also configured to update the weights of the neural network based on the error events. In yet another aspect of the present disclosure an apparatus for training a neural network with back propagation is presented. The apparatus includes means for generating error events representing a gradient of a cost function for the neural network. The error events are generated based on a forward pass through the neural network resulting from input events, weights of the neural network and events from a target signal. The apparatus also includes means for updating the weights of the neural network based on the error events. In still another aspect of the present disclosure, a non-transitory computer-readable medium is presented. The non-transitory computer-readable medium has encoded thereon program code for training a neural network with back propagation. The program code is executed by a processor and includes program code to generate error eve...",
  "date_published": "20170810",
  "title": "SPIKING MULTI-LAYER PERCEPTRON",
  "ipcr_labels": [
    "G06N308",
    "G06F1107"
  ],
  "_processing_info": {
    "original_size": 81729,
    "optimized_size": 3107,
    "reduction_percent": 96.2
  }
}