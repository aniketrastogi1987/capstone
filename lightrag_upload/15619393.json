{
  "patent_number": "None",
  "application_number": "15619393",
  "date_published": "20170928",
  "date_produced": "20170915",
  "filing_date": "20170609",
  "main_ipcr_label": "G06N9900",
  "abstract": "We describe a method of reinforcement learning for a subject system having multiple states and actions to move from one state to the next. Training data is generated by operating on the system with a succession of actions and used to train a second neural network. Target values for training the second neural network are derived from a first neural network which is generated by copying weights of the second neural network at intervals.",
  "publication_number": "US20170278018A1-20170928",
  "summary": "<SOH> SUMMARY OF THE INVENTION <EOH>According to the present invention there is therefore provided a method of reinforcement learning, the method comprising: inputting training data relating to a subject system, the subject system having a plurality of states and, for each state, a set of actions to move from one of said states to a next said state; wherein said training data is generated by operating on said system with a succession of said actions and comprises starting state data, action data and next state data defining, respectively for a plurality of said actions, a starting state, an action, and a next said state resulting from the action; and training a second neural network using said training data and target values for said second neural network derived from a first neural network; the method further comprising: generating or updating said first neural network from said second neural network. Broadly speaking in embodiments of this aspect of the invention two neural networks are maintained to avoid the divergences which can otherwise occur when estimating an action-value parameter, in particular where a neural network would otherwise be updated based on its own predictions. In embodiments, the first neural network generates a target action-value parameter, such as a target Q-value, and the second neural network is updated based on the target generated by the first. In this way the target remains substantially stationary, but at intervals the first neural network is regenerated or updated from the second neural network, for example by copying some or all of a set of weights learnt by the second neural network to the first neural network. In effect, in embodiments, two instances of the same neural network are maintained, a first instance being used to generate the target values for updating the second, from time to time updating the first instance to match the second. Potentially in a locally connected network different portions of the network could be updat...",
  "ipcr_labels": [
    "G06N9900",
    "G06N304",
    "A63F1367"
  ],
  "inventor_list": [
    {
      "inventor_name_last": "Mnih",
      "inventor_name_first": "Volodymyr",
      "inventor_city": "London",
      "inventor_state": "",
      "inventor_country": "GB"
    },
    {
      "inventor_name_last": "Kavukcuoglu",
      "inventor_name_first": "Koray",
      "inventor_city": "London",
      "inventor_state": "",
      "inventor_country": "GB"
    }
  ],
  "title": "METHODS AND APPARATUS FOR REINFORCEMENT LEARNING",
  "decision": "PENDING",
  "_processing_info": {
    "original_size": 97775,
    "optimized_size": 3142,
    "reduction_percent": 96.79
  }
}