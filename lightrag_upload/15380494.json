{
  "date_produced": "20180608",
  "publication_number": "US20180174036A1-20180621",
  "main_ipcr_label": "G06N308",
  "decision": "PENDING",
  "application_number": "15380494",
  "inventor_list": [
    {
      "inventor_name_last": "HAN",
      "inventor_name_first": "Song",
      "inventor_city": "Stanford",
      "inventor_state": "CA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "XIE",
      "inventor_name_first": "Dongliang",
      "inventor_city": "Beijing",
      "inventor_state": "",
      "inventor_country": "CN"
    },
    {
      "inventor_name_last": "KANG",
      "inventor_name_first": "Junlong",
      "inventor_city": "Beijing",
      "inventor_state": "",
      "inventor_country": "CN"
    },
    {
      "inventor_name_last": "LI",
      "inventor_name_first": "Yubin",
      "inventor_city": "Beijing",
      "inventor_state": "",
      "inventor_country": "CN"
    }
  ],
  "abstract": "Hardware accelerator for compressed Long Short Term Memory (LSTM) is disclosed. The accelerator comprise a sparse matrix-vector multiplication module for performing multiplication operation between all sparse matrices in the LSTM and vectors to sequentially obtain a plurality of sparse matrix-vector multiplication results. A addition tree module are also included for accumulating a plurality of said sparse matrix multiplication results to obtain an accumulated result. And a non-linear operation module passes the accumulated results through an activation function to generate non-linear operation result. That is, the present accelerator adopts pipeline design to overlap the time of data transfer and computation for compressed LSTM.",
  "filing_date": "20161215",
  "patent_number": "None",
  "summary": "<SOH> SUMMARY OF THE DISCLOSURE <EOH>This specification describes an efficient hardware architecture for efficiently operating on compressed LSTM. To speed up the prediction and make it energy efficient, the hardware architecture works directly on the compressed LSTM model. The architecture can al so encode and partition the compressed model to each Processing Element (PE) for parallelism, and schedule the complicated LSTM data flow. In a general aspect, a hardware accelerator of LSTM comprises a sparse matrix-vector multiplication module for performing multiplication operation between all sparse matrices in the LSTM and vectors to sequentially obtain a plurality of sparse matrix-vector multiplication results. The hardware accelerator further comprises an addition tree module for accumulating a plurality of said sparse matrix multiplication results to obtain an accumulated result and a non-linear operation module for passing the accumulated results through an activation function to generate non-linear operation result. In another aspect, the hardware accelerator further comprises an element-wise multiplication module for performing element-wise multiplication and addition operations on a plurality of said non-linear operation results to obtain a first intermediate result vector of the current cycle. In another aspect, the hardware accelerator further comprises a diagonal matrix multiplication module for performing a multiplication operation between a diagonal matrix and the first intermediate result vector of the current cycle or the first intermediate result vector of the previous cycle to obtain a diagonal matrix multiplication result. The addition tree module further accumulates the diagonal matrix multiplication results into the accumulated results. In another aspect, the computation and data-fetching of the hardware accelerator can be fully overlapped. The computation process is divided into two layers in parallel: one layer is the multiplication operation of t...",
  "date_published": "20180621",
  "title": "Hardware Accelerator for Compressed LSTM",
  "ipcr_labels": [
    "G06N308",
    "G06N304"
  ],
  "_processing_info": {
    "original_size": 40258,
    "optimized_size": 3696,
    "reduction_percent": 90.82
  }
}