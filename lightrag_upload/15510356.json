{
  "patent_number": "None",
  "application_number": "15510356",
  "date_published": "20171026",
  "date_produced": "20171011",
  "filing_date": "20170310",
  "main_ipcr_label": "G06N308",
  "abstract": "Techniques and constructs can reduce the time required to determine solutions to optimization problems such as training of neural networks. Modifications to a computational model can be determined by a plurality of nodes operating in parallel. Quantized modification values can be transmitted between the nodes to reduce the volume of data to be transferred. The quantized values can be as small as one bit each. Quantization-error values can be stored and used in quantizing subsequent modifications. The nodes can operate in parallel and overlap computation and data transfer to further reduce the time required to determine solutions. The quantized values can be partitioned and each node can aggregate values for a corresponding partition.",
  "publication_number": "US20170308789A1-20171026",
  "summary": "<SOH> SUMMARY <EOH>This disclosure describes systems, methods, and computer-readable media for mathematically optimizing solutions to computational models, e.g., for training deep neural networks (DNNs). In at least one example, each of a plurality of nodes determines modification values of the computational model (e.g., gradient values computed using training data and the DNN model). The nodes quantize the modification values and transmit the quantized values to others of the nodes. An updating module in each node modifies the computational model according to received quantized values. Example techniques described herein determine gradient matrices of the DNN, quantize the gradient matrices using stored error matrices, update the stored error matrices, and exchange the quantized gradient matrices with other nodes. This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter. The term “techniques,” for instance, may refer to system(s), method(s), computer-readable instructions, module(s), algorithms, hardware logic, or operation(s) as permitted by the context described above and throughout the document.",
  "ipcr_labels": [
    "G06N308",
    "G06N700",
    "G06N304"
  ],
  "inventor_list": [
    {
      "inventor_name_last": "LANGFORD",
      "inventor_name_first": "John",
      "inventor_city": "Scarsdale",
      "inventor_state": "NY",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "LI",
      "inventor_name_first": "Gang",
      "inventor_city": "Beijing",
      "inventor_state": "",
      "inventor_country": "CN"
    },
    {
      "inventor_name_last": "SEIDE",
      "inventor_name_first": "Frank Torsten Bernd",
      "inventor_city": "Beijing",
      "inventor_state": "",
      "inventor_country": "CN"
    },
    {
      "inventor_name_last": "DROPPO",
      "inventor_name_first": "James",
      "inventor_city": "Carnation",
      "inventor_state": "WA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "YU",
      "inventor_name_first": "Dong",
      "inventor_city": "Bothell",
      "inventor_state": "WA",
      "inventor_country": "US"
    }
  ],
  "title": "COMPUTING SYSTEM FOR TRAINING NEURAL NETWORKS",
  "decision": "PENDING",
  "_processing_info": {
    "original_size": 109947,
    "optimized_size": 3280,
    "reduction_percent": 97.02
  }
}