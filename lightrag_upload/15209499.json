{
  "date_produced": "20170223",
  "publication_number": "US20170068889A1-20170309",
  "main_ipcr_label": "G06N310",
  "decision": "PENDING",
  "application_number": "15209499",
  "inventor_list": [
    {
      "inventor_name_last": "Fougner",
      "inventor_name_first": "Christopher",
      "inventor_city": "Palo Alto",
      "inventor_state": "CA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Catanzaro",
      "inventor_name_first": "Bryan",
      "inventor_city": "Cupertino",
      "inventor_state": "CA",
      "inventor_country": "US"
    }
  ],
  "abstract": "Disclosed are systems and methods that implement efficient engines for computation-intensive tasks such as neural network deployment. Various embodiments of the invention provide for high-throughput batching that increases throughput of streaming data in high-traffic applications, such as real-time speech transcription. In embodiments, throughput is increased by dynamically assembling into batches and processing together user requests that randomly arrive at unknown timing such that not all the data is present at once at the time of batching. Some embodiments allow for performing steaming classification using pre-processing. The gains in performance allow for more efficient use of a compute engine and drastically reduce the cost of deploying large neural networks at scale, while meeting strict application requirements and adding relatively little computational latency so as to maintain a satisfactory application experience.",
  "filing_date": "20160713",
  "patent_number": "None",
  "summary": "<SOH> BRIEF DESCRIPTION OF THE DRAWINGS <EOH>References will be made to embodiments of the invention, examples of which may be illustrated in the accompanying figures. These figures are intended to be illustrative, not limiting. Although the invention is generally described in the context of these embodiments, it should be understood that it is not intended to limit the scope of the invention to these particular embodiments. Items in the figures are not to scale. FIG. 1 depicts a typical infrastructure configuration for deploying deep neural networks. FIG. 2 depicts an infrastructure configuration for deploying neural networks in accordance with embodiments of the present disclosure. FIG. 3 depicts example server architectures that may be used for deploying neural networks in accordance with embodiments of the present disclosure. FIG. 4 illustrates an exemplary batching method for deploying neural networks to increase throughput of data processing requests in accordance with embodiments of the present disclosure. FIG. 5 illustrates an exemplary batching method using pre-processing in accordance with embodiments of the present disclosure. FIG. 6 presents a graphic that depicts the situation where chunks of data from four streams are processed within 80 ms in a traditional setting. FIG. 7 presents an exemplary graphic that depicts the situation where chunks of data from 20 streams are processed within 80 ms in a batched setting in accordance with embodiments of the present disclosure. FIG. 8 depicts a simplified block diagram of a computing device/information handling system, in accordance with embodiments of the present disclosure. detailed-description description=\"Detailed Description\" end=\"lead\"?",
  "date_published": "20170309",
  "title": "SYSTEMS AND METHODS FOR EFFICIENT NEURAL NETWORK DEPLOYMENTS",
  "ipcr_labels": [
    "G06N310",
    "H04L1224",
    "G06N308"
  ],
  "_processing_info": {
    "original_size": 42607,
    "optimized_size": 3391,
    "reduction_percent": 92.04
  }
}