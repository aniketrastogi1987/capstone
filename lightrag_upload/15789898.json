{
  "patent_number": "None",
  "application_number": "15789898",
  "date_published": "20180426",
  "date_produced": "20180411",
  "filing_date": "20171020",
  "main_ipcr_label": "G06N308",
  "abstract": "A trained computer model includes a direct network and an indirect network. The indirect network generates expected weights or an expected weight distribution for the nodes and layers of the direct network. These expected characteristics may be used to regularize training of the direct network weights and encourage the direct network weights towards those expected, or predicted by the indirect network. Alternatively, the expected weight distribution may be used to probabilistically predict the output of the direct network according to the likelihood of different weights or weight sets provided by the expected weight distribution. The output may be generated by sampling weight sets from the distribution and evaluating the sampled weight sets.",
  "publication_number": "US20180114113A1-20180426",
  "summary": "<SOH> SUMMARY <EOH>A direct mapping for a network is learned in conjunction with an indirect network that designates expected weights for the direct mapping. The network generating the “direct mapping” may also be termed a “direct network” or a “direct model.” The indirect network learns an expected weight distribution of the weights of the direct network, which may be represented as a set of “expected” weights for the direct mapping. The indirect network may also be termed an “indirect model.” The direct model may include a portion of a larger modeled network, such as a multi-node, multi-layered neural network, wherein each direct network models transitions for one or more nodes in the network. The indirect model generates the expected weight distribution based on a set of indirect parameters that affect how the indirect network models the direct network weights. In addition, the indirect model may also be generated based on a control input that describes characteristics of the direct model, such as the particular node, layer, type of input, or other aspect that conditions the generated weights. This indirect model may thus predict more general changes in the weights of the model that vary across the characteristics. The indirect model thus produces “expected” weights and the distribution thereof and that may be used in various ways to improve the direct model for an input to an output. In one approach, the indirect model is used to regularize the weights applied to the mapping in the direct network. In this approach, when training the model, the error term for the direct weights is regularized by the expected weights given by the indirect network. In this way, the indirect network provides an ‘anchor’ or set point from which the direct network weights may vary when it more accurately reflects the data. In this configuration, the regularization term may preference a low difference (and penalizes a high one) between the expected weight and the actual weight of the n...",
  "ipcr_labels": [
    "G06N308",
    "G06N700"
  ],
  "inventor_list": [
    {
      "inventor_name_last": "Ghahramani",
      "inventor_name_first": "Zoubin",
      "inventor_city": "Cambridge",
      "inventor_state": "",
      "inventor_country": "GB"
    },
    {
      "inventor_name_last": "Bemis",
      "inventor_name_first": "Douglas",
      "inventor_city": "San Francisco",
      "inventor_state": "CA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Karaletsos",
      "inventor_name_first": "Theofanis",
      "inventor_city": "San Francisco",
      "inventor_state": "CA",
      "inventor_country": "US"
    }
  ],
  "title": "INTELLIGENT REGULARIZATION OF NEURAL NETWORK ARCHITECTURES",
  "decision": "PENDING",
  "_processing_info": {
    "original_size": 100651,
    "optimized_size": 3690,
    "reduction_percent": 96.33
  }
}