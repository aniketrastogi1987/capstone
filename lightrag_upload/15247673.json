{
  "date_produced": "20180213",
  "publication_number": "US20180060724A1-20180301",
  "main_ipcr_label": "G06N308",
  "decision": "PENDING",
  "application_number": "15247673",
  "inventor_list": [
    {
      "inventor_name_last": "Wang",
      "inventor_name_first": "Changhu",
      "inventor_city": "Beijing",
      "inventor_state": "",
      "inventor_country": "CN"
    },
    {
      "inventor_name_last": "Rui",
      "inventor_name_first": "Yong",
      "inventor_city": "Sammamish",
      "inventor_state": "WA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "Wei",
      "inventor_name_first": "Tao",
      "inventor_city": "Yishui",
      "inventor_state": "",
      "inventor_country": "CN"
    }
  ],
  "abstract": "This disclosure describes techniques and architectures to morph well-trained networks to other related applications or modified networks with relatively little retraining. For example, a well-trained neural network (e.g., parent network) may be morphed to a new neural network (e.g., child network) so that the new neural network function may be preserved. After morphing a parent network, the child network may inherit the knowledge from its parent network and also may have a potential to continue growing into a more powerful network. Such morphing and growing may occur with a relatively short training time.",
  "filing_date": "20160825",
  "patent_number": "None",
  "summary": "<SOH> SUMMARY <EOH>This disclosure describes techniques and architectures to morph well-trained networks to other related applications or modified networks with relatively little retraining. For example, a well-trained neural network (e.g., parent network) may be morphed to a new neural network (e.g., child network) so that the neural network function of the parent network may be preserved in the child network. After morphing a parent network, the child network may inherit the knowledge from its parent network and also may have a potential to continue growing into a more powerful network. Such morphing and growing may occur with a much shortened training time, compared to the case where a redesigned child network is trained from scratch. This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter. The term “techniques,” for instance, may refer to system(s), method(s), computer-readable instructions, module(s), algorithms, hardware logic (e.g., Field-programmable Gate Arrays (FPGAs), Application-specific Integrated Circuits (ASICs), Application-specific Standard Products (ASSPs), System-on-a-chip systems (SOCs), Complex Programmable Logic Devices (CPLDs)), quantum devices, such as quantum computers or quantum annealers, and/or other technique(s) as permitted by the context above and throughout the document.",
  "date_published": "20180301",
  "title": "Network Morphism",
  "ipcr_labels": [
    "G06N308",
    "G06N502"
  ],
  "_processing_info": {
    "original_size": 59685,
    "optimized_size": 3006,
    "reduction_percent": 94.96
  }
}