{
  "patent_number": "None",
  "application_number": "15420801",
  "date_published": "20180510",
  "date_produced": "20180425",
  "filing_date": "20170131",
  "main_ipcr_label": "G06N304",
  "abstract": "The technology disclosed provides a quasi-recurrent neural network (QRNN) encoder-decoder model that alternates convolutional layers, which apply in parallel across timesteps, and minimalist recurrent pooling layers that apply in parallel across feature dimensions.",
  "publication_number": "US20180129931A1-20180510",
  "summary": "<SOH> BRIEF DESCRIPTION OF THE DRAWINGS <EOH>In the drawings, like reference characters generally refer to like parts throughout the different views. Also, the drawings are not necessarily to scale, with an emphasis instead generally being placed upon illustrating the principles of the technology disclosed. In the following description, various implementations of the technology disclosed are described with reference to the following drawings, in which: FIG. 1 illustrates aspects of a quasi-recurrent neural network (QRNN) that increases computational efficiency in natural language processing (NLP) tasks. FIG. 2 shows one implementation of a convolutional layer that operates in parallel over a time series of input vectors and concurrently outputs convolutional vectors. FIG. 3 depicts one implementation of a convolutional vector comprising an activation vector, a forget gate vector, an input gate vector, and an output gate vector. FIG. 4 is one implementation of multiple convolutional vectors, and comprising activation vectors and gate vectors, concurrently outputted by a convolutional layer. FIG. 5 illustrates one implementation of feature values at ordinal positions in activation vectors and gate vectors concurrently outputted by a convolutional layer. FIG. 6 is one implementation of a single-gate pooling layer that applies accumulators in parallel to concurrently accumulate an ordered set of feature sums in a state vector, and sequentially outputs successive state vectors. FIG. 7 illustrates one implementation a multi-gate pooling layer that applies accumulators in parallel to concurrently accumulate an ordered set of feature sums in a state vector, and sequentially outputs successive state vectors. FIG. 8 depicts one implementation of successive state vectors sequentially outputted by a pooling layer. FIG. 9 is one implementation of a QRNN encoder-decoder model. FIG. 10 is a table that shows accuracy comparison of the QRNN on sentiment classification task. FIG. 11 ...",
  "ipcr_labels": [
    "G06N304"
  ],
  "inventor_list": [
    {
      "inventor_name_last": "BRADBURY",
      "inventor_name_first": "James",
      "inventor_city": "San Francisco",
      "inventor_state": "CA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "MERITY",
      "inventor_name_first": "Stephen Joseph",
      "inventor_city": "San Francisco",
      "inventor_state": "CA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "XIONG",
      "inventor_name_first": "Caiming",
      "inventor_city": "Palo Alto",
      "inventor_state": "CA",
      "inventor_country": "US"
    },
    {
      "inventor_name_last": "SOCHER",
      "inventor_name_first": "Richard",
      "inventor_city": "Menlo Park",
      "inventor_state": "CA",
      "inventor_country": "US"
    }
  ],
  "title": "QUASI-RECURRENT NEURAL NETWORK BASED ENCODER-DECODER MODEL",
  "decision": "PENDING",
  "_processing_info": {
    "original_size": 81253,
    "optimized_size": 3272,
    "reduction_percent": 95.97
  }
}